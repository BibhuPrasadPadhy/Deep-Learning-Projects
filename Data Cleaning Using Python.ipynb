{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b350452e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8a828785",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('indexData.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "98c3f03e",
   "metadata": {},
   "source": [
    "## Missing Value\n",
    "\n",
    "#### Various variants of Missing Values:\n",
    "1. Missing completely at random(MCAR) – Data is missing due to mishandling of data, Can’t be predicted from rest of the variables. Example : Data lost during ETL process, (Similar Case Imputation might not work here)\n",
    "2. Missing at random(MAR) – Some fields which are not relevant to a certain sample of population would be missing. Example : whether a group of older people (age > 45) will be using ZEE5,Prime Video or not. (Similar Case Imputation might work here)\n",
    "3. Missing not at random(MNAR) – If we have data that is not missing randomly but had been intentionally left empty. Example : Sick people leaving a field which asks for alcohol (Yes/No), would rather choose to leave the field empty. (Tricky Situation might have to evaluate different options while filling missing data).\n",
    "\n",
    "#### How to handle Missing data (Approach) : \n",
    "1. Find the columns where data is missing with respective percentage.\n",
    "2. If less than 20% data is missing, First try to find the reason why the data is missing, they might be missing due to a single reason or due to different reasons, But if more than 20% data is missing for any attribute, we might need to discuss with stakeholders to check if we can drop the column or we can somehow enrich that attribute.\n",
    "3. Post finding the reason of missing, check if the data can be enriched from source (If the data was missing due to mishandling).\n",
    "4. Post step 3, If we have missing values left, we can use methods listed in the next slide."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca57d75",
   "metadata": {},
   "source": [
    "#### Number of Missing values in absolute numbers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "489a5e7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index           0\n",
       "Date            0\n",
       "Open         2204\n",
       "High         2204\n",
       "Low          2204\n",
       "Close        2204\n",
       "Adj Close    2204\n",
       "Volume       2204\n",
       "dtype: int64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3707b080",
   "metadata": {},
   "source": [
    "#### Number of missing value in %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9104c583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index        0.00000\n",
       "Date         0.00000\n",
       "Open         1.95986\n",
       "High         1.95986\n",
       "Low          1.95986\n",
       "Close        1.95986\n",
       "Adj Close    1.95986\n",
       "Volume       1.95986\n",
       "dtype: float64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df.isna().sum()/df.shape[0]) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1096a373",
   "metadata": {},
   "source": [
    "#### Number of fill value in % or Fill Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f8fb8046",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index        100.00000\n",
       "Date         100.00000\n",
       "Open          98.04014\n",
       "High          98.04014\n",
       "Low           98.04014\n",
       "Close         98.04014\n",
       "Adj Close     98.04014\n",
       "Volume        98.04014\n",
       "dtype: float64"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((1 - df.isna().sum()/df.shape[0]) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a63f20",
   "metadata": {},
   "source": [
    "##### Conclusion : As we see we have almost 98+ %age of data present, we need not drop any attribute . i.e, Dropping of rows/columns is not needed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4071407",
   "metadata": {},
   "source": [
    "## Outlier Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "88afb6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outlier(df_in, col_name):\n",
    "    q1 = df_in[col_name].quantile(0.25)\n",
    "    q3 = df_in[col_name].quantile(0.75)\n",
    "    iqr = q3-q1 #Interquartile range\n",
    "    fence_low  = q1-1.5*iqr\n",
    "    fence_high = q3+1.5*iqr\n",
    "    df_out = df_in.loc[(df_in[col_name] > fence_low) & (df_in[col_name] < fence_high)]\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a812d220",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.copy()\n",
    "df2 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "25cfb27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = remove_outlier(df1,'Open')\n",
    "df1 = remove_outlier(df1,'High')\n",
    "df1 = remove_outlier(df1,'Low')\n",
    "df1 = remove_outlier(df1,'Close')\n",
    "df1 = remove_outlier(df1,'Adj Close')\n",
    "df1 = remove_outlier(df1,'Volume')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6cbf6377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records dropped which were considered as outlier in IQR Based Approach : 27797\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of records dropped which were considered as outlier in IQR Based Approach : {df.shape[0] - df1.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6995841b",
   "metadata": {},
   "source": [
    "##### Several Other Methods include : \n",
    "\n",
    "1. Outlier Removal using Pyod(Python Package) \n",
    "2. Z-Score Method of Outlier Filtering : Filter out all the data points which are beyond (Mean     3* Standard Deviation) , As we expect 99.7% records failing in this range. i.e., Z-Score > 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d93f6d6",
   "metadata": {},
   "source": [
    "#### Single Imputation Method : \n",
    "\n",
    "1. Imputation with Mean – If the distribution is normal.\n",
    "2. Imputation with Median – If the distribution is not normal\n",
    "3. Imputation with Mode – If the data is categorical in nature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b57c531",
   "metadata": {},
   "source": [
    "#### creating a dataset where we have missing categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ace41847",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[50:55,'Index'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e160ecaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index        6\n",
       "Date         0\n",
       "Open         0\n",
       "High         0\n",
       "Low          0\n",
       "Close        0\n",
       "Adj Close    0\n",
       "Volume       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f010e4e5",
   "metadata": {},
   "source": [
    "#### Imputation with Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8662c95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Index'] = df['Index'].fillna(df['Index'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "873acc21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index        0\n",
       "Date         0\n",
       "Open         0\n",
       "High         0\n",
       "Low          0\n",
       "Close        0\n",
       "Adj Close    0\n",
       "Volume       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d91bc2e",
   "metadata": {},
   "source": [
    "#### Check if we can replace with Mean, As we have removed outliers in df1, we can use df1 to replace with mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "94274575",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_cols = ['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']\n",
    "\n",
    "for col in missing_cols:\n",
    "    df1[col].fillna(df1[col].mean(),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "cc5ff32d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index        0\n",
       "Date         0\n",
       "Open         0\n",
       "High         0\n",
       "Low          0\n",
       "Close        0\n",
       "Adj Close    0\n",
       "Volume       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7591847a",
   "metadata": {},
   "source": [
    "#### But for Df we did not remove the outliers , so we can't replace with Mean but with Median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "02ff77e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in missing_cols:\n",
    "    df[col].fillna(df[col].median(),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "521a58cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index        0\n",
       "Date         0\n",
       "Open         0\n",
       "High         0\n",
       "Low          0\n",
       "Close        0\n",
       "Adj Close    0\n",
       "Volume       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237891c1",
   "metadata": {},
   "source": [
    "#### For stock price missing data, we recommend forward fill & Backward Fill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d7d5f5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.ffill(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "348a3793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index        0\n",
       "Date         0\n",
       "Open         0\n",
       "High         0\n",
       "Low          0\n",
       "Close        0\n",
       "Adj Close    0\n",
       "Volume       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e288758",
   "metadata": {},
   "source": [
    "#### Dropping duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a533d035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Considering all the columns and dropping duplicates\n",
    "df2.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3c947e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Considering only few columns and dropping duplicates\n",
    "df2.drop_duplicates(subset=['Index','Date'],keep='first',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f05ac4",
   "metadata": {},
   "source": [
    "#### Data Format Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "02bc4f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inserting a erroneous date\n",
    "df.loc[33,'Date'] = '202212032'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "2c2c226d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the records which are not matching %Y-%m-%d datetime format\n",
    "indexes = pd.to_datetime(df['Date'], format='%Y-%m-%d', errors='coerce').isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "d00fe5df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33    202212032\n",
       "Name: Date, dtype: object"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[indexes,'Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "bca6060a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[indexes,'Date'] = pd.to_datetime(datetime(2022,12,3), format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "0c4219e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = pd.to_datetime(df['Date'], format='%Y-%m-%d', errors='coerce').isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "269ebb12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: Date, dtype: object)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[indexes,'Date']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
